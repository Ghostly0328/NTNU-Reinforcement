{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "import gym\n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "num_episodes = 10000\n",
    "gamma = 1.0\n",
    "epsilon = 0.1\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, Q):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(env.action_space.n)\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "def sarsa_update_value(state, action, reward, next_state, next_action, done, Q):\n",
    "    if done:\n",
    "        Q[state][action] += alpha * (reward - Q[state][action])\n",
    "    else:\n",
    "        Q[state][action] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])\n",
    "\n",
    "def run_sarsa(num_episodes, render = False):\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    total_reward = []\n",
    "    for i in range(num_episodes):\n",
    "        rewards = 0\n",
    "        state = env.reset()\n",
    "        state = state[0]\n",
    "        action = choose_action(state, Q)\n",
    "        while True:\n",
    "            if i == num_episodes - 1 and render:\n",
    "                env.render()\n",
    "            next_state, reward, done, MDP, info = env.step(action)\n",
    "            next_action = choose_action(next_state, Q)\n",
    "            sarsa_update_value(state, action, reward, next_state, next_action, done, Q)\n",
    "            rewards += reward\n",
    "            if done:\n",
    "                break\n",
    "            state, action = next_state, next_action\n",
    "        total_reward.append(rewards)\n",
    "        print(f'\\repisode: {i + 1}/{num_episodes}', end = '')\n",
    "        sys.stdout.flush()\n",
    "    print(f'\\nMax Reward: {max(total_reward)}')\n",
    "    return total_reward\n",
    "   \n",
    "run_sarsa(num_episodes, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_update_value(state, action, reward, next_state, done, Q):\n",
    "    if done:\n",
    "        Q[state][action] += alpha * (reward - Q[state][action])\n",
    "    else:\n",
    "        Q[state][action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][action])\n",
    "def run_q_learning(num_episodes, render = False):\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    total_reward = []\n",
    "    for i in range(num_episodes):\n",
    "        rewards = 0\n",
    "        state = env.reset()\n",
    "        state = state[0]\n",
    "        while True:\n",
    "            if i == num_episodes - 1 and render:\n",
    "                env.render()\n",
    "            action = choose_action(state, Q)\n",
    "            next_state, reward, done, MDP, info = env.step(action)\n",
    "            q_update_value(state, action, reward, next_state, done, Q)\n",
    "            rewards += reward\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        total_reward.append(rewards)\n",
    "        print(f'\\repisode: {i + 1}/{num_episodes}', end = '')\n",
    "        sys.stdout.flush()\n",
    "    print(f'\\nMax Reward: {max(total_reward)}')\n",
    "    return total_reward\n",
    "    \n",
    "run_q_learning(num_episodes, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_sarsa_update_value(state, action, reward, next_state, done, Q):\n",
    "    if done:\n",
    "        Q[state][action] += alpha * (reward - Q[state][action])\n",
    "    else:\n",
    "        policy = np.ones(env.action_space.n) * epsilon / env.action_space.n\n",
    "        policy[np.argmax(Q[next_state])] += 1 - epsilon\n",
    "        Q[state][action] += alpha * (reward + gamma * np.dot(policy, Q[next_state]) - Q[state][action])\n",
    "def run_expected_sarsa(num_episodes, render = False):\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    total_reward = []\n",
    "    for i in range(num_episodes):\n",
    "        rewards = 0\n",
    "        state = env.reset()\n",
    "        state = state[0]\n",
    "        while True:\n",
    "            if i == num_episodes - 1 and render:\n",
    "                env.render()\n",
    "            action = choose_action(state, Q)\n",
    "            next_state, reward, done, MDP, info = env.step(action, Q)\n",
    "            expected_sarsa_update_value(state, action, reward, next_state, done, Q)\n",
    "            rewards += reward\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        total_reward.append(rewards)\n",
    "        print(f'\\repisode: {i + 1}/{num_episodes}', end = '')\n",
    "        sys.stdout.flush()\n",
    "    print(f'\\nMax Reward: {max(total_reward)}')\n",
    "    return total_reward\n",
    "    \n",
    "run_expected_sarsa(num_episodes, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "sarsa_reward = [0 for i in range(500)]\n",
    "expected_sarsa_reward = [0 for i in range(500)]\n",
    "q_reward = [0 for i in range(500)]\n",
    "for i in range(100):\n",
    "    sarsa_reward = [sum(x) for x in zip(sarsa_reward, run_sarsa(1000))]\n",
    "    expected_sarsa_reward = [sum(x) for x in zip(expected_sarsa_reward, run_expected_sarsa(1000))]\n",
    "    q_reward = [sum(x) for x in zip(q_reward, run_q_learning(1000))]\n",
    "    \n",
    "sarsa_reward = np.array(sarsa_reward) / 100\n",
    "expected_sarsa_reward = np.array(expected_sarsa_reward) / 100\n",
    "q_reward = np.array(q_reward) / 100\n",
    "plt.plot(sarsa_reward)\n",
    "plt.plot(expected_sarsa_reward)\n",
    "plt.plot(q_reward)\n",
    "plt.legend(['sarss reward', 'expected sarsa reward', 'q learning reward'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "env.action_space.seed(42)\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "for _ in range(1000):\n",
    "    observation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
