{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "env = env.unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194, {'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})\n"
     ]
    }
   ],
   "source": [
    "#遊戲重新開機\n",
    "print(env.reset())\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#遊戲關機\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "500\n",
      "500\n",
      "{0: [(1.0, 100, -1, False)], 1: [(1.0, 0, -1, False)], 2: [(1.0, 20, -1, False)], 3: [(1.0, 0, -1, False)], 4: [(1.0, 16, -1, False)], 5: [(1.0, 0, -10, False)]}\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.n) # 6\n",
    "print(env.observation_space.n) # 500\n",
    "\n",
    "print(len(env.P)) # 500\n",
    "\n",
    "print(env.P[0]) \n",
    "# {\n",
    "#  0: [(1.0, 100, -1, False)],\n",
    "#  1: [(1.0, 0, -1, False)],\n",
    "#  2: [(1.0, 20, -1, False)],\n",
    "#  3: [(1.0, 0, -1, False)],\n",
    "#  4: [(1.0, 16, -1, False)],\n",
    "#  5: [(1.0, 0, -10, False)]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375,\n",
       " -1,\n",
       " False,\n",
       " False,\n",
       " {'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1) # choose action 0\n",
    "#state, reward, terminal state, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward: 5, steps: 16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def policy_evaluation(V, policy, gamma, theta):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            v = 0\n",
    "            for action, action_prob in enumerate(policy[s]):\n",
    "                for prob, next_state, reward, done in env.P[s][action]:\n",
    "                    if done:\n",
    "                        v += action_prob * prob * reward\n",
    "                    else:\n",
    "                        v += (action_prob * prob * (reward + gamma * V[next_state]))\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "def policy_improvent(V, policy, gamma):\n",
    "    policy_stable = True\n",
    "    for s in range(env.observation_space.n):\n",
    "        old_policy = policy[s].copy()\n",
    "        q_s = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                if done:\n",
    "                    q_s[a] += prob * reward\n",
    "                else:\n",
    "                    q_s[a] += (prob * (reward + gamma * V[next_state]))\n",
    "        best_action = np.argmax(q_s)\n",
    "        policy[s] = np.eye(env.action_space.n)[best_action]\n",
    "        if np.any(old_policy != policy[s]):\n",
    "            policy_stable = False\n",
    "    return policy_stable\n",
    "\n",
    "def policy_iteration(gamma = 0.99, max_iteration = 10000, theta = 0.0001):\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    policy = np.ones([env.observation_space.n, env.action_space.n]) / env.action_space.n\n",
    "    for i in range(max_iteration):  \n",
    "        policy_evaluation(V, policy, gamma, theta)\n",
    "        stable = policy_improvent(V, policy, gamma)\n",
    "        if stable:\n",
    "            break\n",
    "    return V, policy\n",
    "\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "env = env.unwrapped\n",
    "\n",
    "V, policy = policy_iteration(gamma = 0.1, max_iteration = 10000, theta = 0.0001)\n",
    "state = env.reset()\n",
    "state = state[0]\n",
    "rewards = 0\n",
    "steps = 0\n",
    "while True:\n",
    "    env.render()\n",
    "    action = np.argmax(policy[state])\n",
    "    next_state, reward, done, MDP, info = env.step(action)\n",
    "    rewards += reward\n",
    "    steps += 1\n",
    "    if done:\n",
    "        break\n",
    "    state = next_state\n",
    "print(f'total reward: {rewards}, steps: {steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward: 10, steps: 11\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(gamma, max_iteration, theta):\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    for i in range(max_iteration):\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            q_s = np.zeros(env.action_space.n)\n",
    "            for a in range(env.action_space.n):\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    if done:\n",
    "                        q_s[a] += prob * reward\n",
    "                    else:\n",
    "                        q_s[a] += (prob * (reward + gamma * V[next_state]))\n",
    "            best_value = max(q_s)\n",
    "            delta = max(delta, np.abs(best_value - V[s]))\n",
    "            V[s] = best_value\n",
    "        if delta < theta:\n",
    "            break\n",
    "    policy = get_policy(V, gamma)\n",
    "    return V, policy\n",
    "    \n",
    "def get_policy(V, gamma):\n",
    "    policy = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for s in range(env.observation_space.n):\n",
    "        q_s = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                if done:\n",
    "                    q_s[a] += prob * reward\n",
    "                else:\n",
    "                    q_s[a] += (prob * (reward + gamma * V[next_state]))\n",
    "        best_action = np.argmax(q_s)\n",
    "        policy[s] = np.eye(env.action_space.n)[best_action]\n",
    "    return policy\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "env = env.unwrapped\n",
    "\n",
    "V, policy = value_iteration(gamma = 1.0, max_iteration = 10000, theta = 0.0001)\n",
    "state = env.reset()\n",
    "state = state[0]\n",
    "rewards = 0\n",
    "steps = 0\n",
    "while True:\n",
    "    env.render()\n",
    "    action = np.argmax(policy[state])\n",
    "    next_state, reward, done, MDP, info = env.step(action)\n",
    "    rewards += reward\n",
    "    steps += 1\n",
    "    if done:\n",
    "        break\n",
    "    state = next_state\n",
    "print(f'total reward: {rewards}, steps: {steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#Compare\n",
    "p1 = policy_iteration(gamma = 0.99, max_iteration = 10000, theta = 0.0001)[1] \n",
    "p2 = value_iteration(gamma = 1.0, max_iteration = 10000, theta = 0.0001)[1] \n",
    "print(np.all(p1 == p2)) # True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
